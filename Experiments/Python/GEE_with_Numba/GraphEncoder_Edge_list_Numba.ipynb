{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Numba"
      ],
      "metadata": {
        "id": "3TsGU1EpgORN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. https://colab.research.google.com/github/cbernet/maldives/blob/master/numba/numba_cuda.ipynb#scrollTo=Ctr6aM3cmkdx\n",
        "\n",
        "2. https://stackoverflow.com/questions/48811248/how-to-use-numba-in-colaboratory"
      ],
      "metadata": {
        "id": "Yc-AuJkZkAu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -iname 'libdevice'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e8Z0qM8fB1M",
        "outputId": "4ff39757-ec01-47ad-8331-e86cb8b21c50"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jaxlib/cuda/nvvm/libdevice\n",
            "/usr/local/cuda-11.2/nvvm-prev/libdevice\n",
            "/usr/local/cuda-11.2/nvvm/libdevice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -iname 'libnvvm.so'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOzI8jmDgJ-M",
        "outputId": "29e57404-50c7-468f-be64-8f7a1ff82244"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/proc/45/task/45/net’: Invalid argument\n",
            "find: ‘/proc/45/net’: Invalid argument\n",
            "/usr/local/cuda-11.2/nvvm-prev/lib64/libnvvm.so\n",
            "/usr/local/cuda-11.2/nvvm/lib64/libnvvm.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-11.2/nvvm/libdevice\"\n",
        "os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-11.2/nvvm-prev/lib64/libnvvm.so\""
      ],
      "metadata": {
        "id": "a-Jyus7jf-mM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import jit"
      ],
      "metadata": {
        "id": "R_FSUJU7hAOn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdFl3x2SBBFf"
      },
      "source": [
        "#Package Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1feGDRiBv-2-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import copy\n",
        "from numpy import linalg as LA\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import metrics\n",
        "import time\n",
        "# for sparse matrix\n",
        "from scipy import sparse\n",
        "#early stop\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N9L480srX8a"
      },
      "source": [
        "#Classes and functions with Numba Decorator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QpkZjKLAskam"
      },
      "outputs": [],
      "source": [
        "# invalide devide resutls will be nan\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "############------------graph_encoder_embed_start----------------###############\n",
        "\n",
        "def GEE_run(Y, n, X=np.array([[]]), **kwargs):\n",
        "    defaultKwargs = {'DiagA': True, 'Laplacian': False, 'Correlation': True}\n",
        "    kwargs = { **defaultKwargs, **kwargs}\n",
        "    \n",
        "    X = to_s3_list(X)\n",
        "    \n",
        "    emb_strat = time.time()\n",
        "\n",
        "    if kwargs['DiagA']:\n",
        "      X = Diagonal(X, n)\n",
        "\n",
        "    if kwargs['Laplacian']:\n",
        "      X = Laplacian(X, n)\n",
        "    \n",
        "    Z, W = GEE_Basic(X, Y, n)\n",
        "\n",
        "    if kwargs['Correlation']:\n",
        "      Z = Correlation(Z)\n",
        "    \n",
        "    emb_end = time.time()\n",
        "    emb_time = emb_end - emb_strat\n",
        "    \n",
        "    return Z, W, emb_time\n",
        "\n",
        "@jit(nopython = True)\n",
        "def GEE_Basic(X, Y, n):\n",
        "  \"\"\"\n",
        "    graph embedding basic function\n",
        "    input X is S3 edge list\n",
        "    input Y is numpy array with size (n,1):\n",
        "    -- value -1 indicate no lable\n",
        "    -- value >=0 indicate real label\n",
        "    input n: number of vertices\n",
        "  \"\"\"\n",
        "  k = Y[:,0].max() + 1\n",
        "\n",
        "  Z = np.zeros((n,k))   \n",
        "  #nk: 1*n array, contains the number of observations in each class\n",
        "  #W: encoder marix. W[i,k] = {1/nk if Yi==k, otherwise 0}\n",
        "  nk = np.zeros((1,k))\n",
        "  W = np.zeros((n,k))\n",
        "\n",
        "  for i in range(k):\n",
        "    nk[0,i] = np.count_nonzero(Y[:,0]==i)\n",
        "\n",
        "  for i in range(Y.shape[0]):\n",
        "    k_i = Y[i,0]\n",
        "    if k_i >=0:\n",
        "      W[i,k_i] = 1/nk[0,k_i] ## GEE paper\n",
        "      # W[i,k_i] = nk[0,k_i]/n ## use as an example to show if people want to use nk/n instead of 1/nk\n",
        "\n",
        "  for row in X:\n",
        "    [v_i, v_j, edg_i_j] = row\n",
        "    v_i = int(v_i)\n",
        "    v_j = int(v_j)\n",
        "\n",
        "    label_i = Y[v_i][0] \n",
        "    label_j = Y[v_j][0]\n",
        "\n",
        "    if label_j >= 0:\n",
        "      Z[v_i, label_j] = Z[v_i, label_j] + W[v_j, label_j]*edg_i_j\n",
        "    if (label_i >= 0) and (v_i != v_j):\n",
        "      Z[v_j, label_i] = Z[v_j, label_i] + W[v_i, label_i]*edg_i_j\n",
        "\n",
        "  return Z, W    \n",
        "  \n",
        "\n",
        "def Diagonal(X, n):\n",
        "  # add self-loop to edg list -- add 1 connection for each (i,i)\n",
        "  self_loops = np.column_stack((np.arange(n), np.arange(n), np.ones(n)))\n",
        "  # faster than vstack --  adding the second to the bottom\n",
        "  X = np.concatenate((X,self_loops), axis = 0)\n",
        "  return X\n",
        "\n",
        "def Laplacian(X, n):\n",
        "  s = X.shape[0] # get the row number of the edg list\n",
        "\n",
        "  D = np.zeros((n,1))\n",
        "  for row in X:\n",
        "    [v_i, v_j, edg_i_j] = row\n",
        "    v_i = int(v_i)\n",
        "    v_j = int(v_j)\n",
        "    D[v_i] = D[v_i] + edg_i_j\n",
        "    if v_i != v_j:\n",
        "      D[v_j] = D[v_j] + edg_i_j\n",
        "\n",
        "  D = np.power(D, -0.5)\n",
        "  \n",
        "  for i in range(s):\n",
        "    X[i,2] = X[i,2] * D[int(X[i,0])] * D[int(X[i,1])]\n",
        "\n",
        "  return X\n",
        "\n",
        "def Correlation(Z):\n",
        "  \"\"\"\n",
        "    Calculate each row's 2-norm (Euclidean distance). \n",
        "    e.g.row_x: [ele_i,ele_j,ele_k]. norm2 = sqr(sum(ele_i^2+ele_i^2+ele_i^2))\n",
        "    then divide each element by their row norm\n",
        "    e.g. [ele_i/norm2,ele_j/norm2,ele_k/norm2]\n",
        "  \"\"\"\n",
        "  row_norm = LA.norm(Z, axis = 1)\n",
        "  reshape_row_norm = np.reshape(row_norm, (n,1))\n",
        "  Z = np.nan_to_num(Z/reshape_row_norm)\n",
        "\n",
        "  return Z\n",
        "\n",
        "\n",
        "def to_s3_list(X):\n",
        "  \"\"\"\n",
        "    if input X only has 2 columns, make it into s3 edge list.\n",
        "    this function will return a s3 edge list\n",
        "    [node_i, node_j, weight]...\n",
        "  \"\"\"\n",
        "  s = X.shape[0] # get the row number of the edg list\n",
        "  if X.shape[1] == 2:\n",
        "    # enlarge the edgelist to s*3 by adding 1 to the thrid position as adj(i,j)\n",
        "    X = np.insert(X, 1, np.ones(s,1))\n",
        "\n",
        "  return X\n",
        "\n",
        "def adj_to_edg(A):\n",
        "  \"\"\"\n",
        "    input is the symmetric adjacency matrix: A\n",
        "    other variables in this function:\n",
        "    s: number of edges\n",
        "    return edg_list -- matrix format with shape(edg_sum,3):\n",
        "    example row in edg_list(matrix): [vertex1, vertex2, connection weight from Adj matrix]\n",
        "  \"\"\"\n",
        "  # check the len of the second dimenson of A\n",
        "  if A.shape[1] <= 3:\n",
        "    edg = A\n",
        "  else:\n",
        "    n = A.shape[0]\n",
        "    # construct the initial edgg_list matrix with the size of (edg_sum, 3)\n",
        "    edg_list = []\n",
        "    for i in range(n):\n",
        "      for j in range(i, n):\n",
        "        if A[i,j] > 0:\n",
        "          row = [i, j, A[i,j]]\n",
        "          edg_list.append(row)\n",
        "    edg = np.array(edg_list)\n",
        "  return edg\n",
        "\n",
        "############------------graph_encoder_embed_end------------------###############\n",
        "############------------Sparse_supervised_learning_start---------###############\n",
        "\n",
        "# https://www.kaggle.com/c/talkingdata-mobile-user-demographics/discussion/22567\n",
        "# https://github.com/tkipf/pygcn/blob/1600b5b748b3976413d1e307540ccc62605b4d6d/pygcn/utils.py#L73\n",
        "\n",
        "def batch_generator(X, y, k, batch_size, shuffle):\n",
        "    number_of_batches = int(X.shape[0]/batch_size)\n",
        "    counter = 0\n",
        "    sample_index = np.arange(X.shape[0])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "    while True:\n",
        "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
        "        X_batch = X[batch_index,:]\n",
        "        y_batch = to_categorical(y[batch_index], num_classes=k)\n",
        "        counter += 1\n",
        "        yield X_batch, y_batch\n",
        "        if (counter == number_of_batches):\n",
        "            if shuffle:\n",
        "                np.random.shuffle(sample_index)\n",
        "            counter = 0\n",
        "\n",
        "class Hyperperameters:\n",
        "  \"\"\"\n",
        "    define perameters for GNN.\n",
        "    default values are for GNN learning -- \"Leaner\" ==2:\n",
        "      embedding via partial label, then learn unknown label via two-layer NN\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # there is no scaled conjugate gradiant in keras optimiser, use defualt instead\n",
        "    # use whatever default\n",
        "    self.learning_rate = 0.01  # Initial learning rate.\n",
        "    self.epochs = 100 #Number of epochs to train.\n",
        "    self.hidden = 20 #Number of units in hidden layer \n",
        "    self.val_split = 0.1 #Split 10% of training data for validation\n",
        "    self.loss = 'categorical_crossentropy' # loss function\n",
        "\n",
        "class GNN:\n",
        "  def __init__(self, DataSets):\n",
        "    GNN.DataSets = DataSets\n",
        "    GNN.hyperM = Hyperperameters()\n",
        "    GNN.model = self.GNN_model()  #model summary: GNN.model.summary()\n",
        "      \n",
        " \n",
        "  def GNN_model(self):\n",
        "    \"\"\"\n",
        "      build GNN model\n",
        "    \"\"\"\n",
        "    hyperM = self.hyperM\n",
        "    DataSets = self.DataSets\n",
        "\n",
        "    z_train = DataSets.z_train\n",
        "    k = DataSets.d\n",
        "\n",
        "    feature_num = z_train.shape[1]\n",
        "    \n",
        "    model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape = (feature_num,)),  # input layer \n",
        "    keras.layers.Dense(hyperM.hidden, activation='relu'),  # hidden layer -- no tansig activation function in Keras, use relu instead\n",
        "    keras.layers.Dense(k, activation='softmax') # output layer, matlab used softmax for patternnet default ??? max(opts.neuron,K)? opts \n",
        "    ])\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = hyperM.learning_rate)\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=hyperM.loss,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "    \n",
        "  def GNN_run(self, flag):\n",
        "    \"\"\"\n",
        "      Train and test directly.\n",
        "      Do not learn from the unknown labels.\n",
        "    \"\"\"\n",
        "    gnn = copy.deepcopy(self)\n",
        "    hyperM = gnn.hyperM\n",
        "    DataSets = self.DataSets\n",
        "    k = DataSets.d\n",
        "    z_train = DataSets.z_train\n",
        "    y_train = DataSets.y_train\n",
        "    y_test = DataSets.y_test\n",
        "    z_test = DataSets.z_test\n",
        "    model = gnn.model    \n",
        "\n",
        "\n",
        "    if flag == \"direct\":\n",
        "      y_train_one_hot = to_categorical(y_train, num_classes=k)\n",
        "      train_strat = time.time() \n",
        "      history = model.fit(z_train, y_train_one_hot, \n",
        "        validation_split=hyperM.val_split,\n",
        "        epochs=hyperM.epochs, \n",
        "        shuffle=True,\n",
        "        verbose=0)\n",
        "    else:\n",
        "      early_stopping_callback = EarlyStopping(monitor='loss', patience=5, verbose=0)\n",
        "      checkpoint_callback = ModelCheckpoint('GNN.h5', monitor='loss', save_best_only=True, mode='min', verbose=0)\n",
        "      \n",
        "      train_strat = time.time()\n",
        "      history = model.fit(batch_generator(z_train, y_train, k, 32, True),\n",
        "                      epochs=hyperM.epochs,\n",
        "                      steps_per_epoch=z_train.shape[0],\n",
        "                      callbacks=[early_stopping_callback, checkpoint_callback],\n",
        "                      verbose=0)\n",
        "    train_end = time.time()\n",
        "    train_time = train_end - train_strat \n",
        "\n",
        "    y_test_one_hot = to_categorical(y_test, num_classes=k) \n",
        "    # set verbose to 0 to silent the output\n",
        "    test_loss, test_acc = gnn.model.evaluate(z_test,  y_test_one_hot, verbose=0) \n",
        "    return test_acc, train_time\n",
        "############------------Sparse_supervised_learning_end---------###############\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classes and functions(original)"
      ],
      "metadata": {
        "id": "rdtkqKg_nhmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# invalide devide resutls will be nan\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "############------------graph_encoder_embed_start----------------###############\n",
        "class GraphEncoderEmbed:\n",
        "  def run(self, X, Y, n, **kwargs):\n",
        "    defaultKwargs = {'DiagA': True, 'Laplacian': False, 'Correlation': True}\n",
        "    kwargs = { **defaultKwargs, **kwargs}\n",
        "    \n",
        "    X = self.to_s3_list(X)\n",
        "    \n",
        "    emb_strat = time.time()\n",
        "\n",
        "    if kwargs['DiagA']:\n",
        "      X = self.Diagonal(X, n)\n",
        "\n",
        "    if kwargs['Laplacian']:\n",
        "      X = self.Laplacian(X, n)\n",
        "    \n",
        "    Z, W = self.Basic(X, Y, n)\n",
        "\n",
        "    if kwargs['Correlation']:\n",
        "      Z = self.Correlation(Z)\n",
        "    \n",
        "    emb_end = time.time()\n",
        "    emb_time = emb_end - emb_strat\n",
        "    \n",
        "    return Z, W, emb_time\n",
        "\n",
        "  def Basic(self, X, Y, n):\n",
        "    \"\"\"\n",
        "      graph embedding basic function\n",
        "      input X is S3 edge list\n",
        "      input Y is numpy array with size (n,1):\n",
        "      -- value -1 indicate no lable\n",
        "      -- value >=0 indicate real label\n",
        "      input n: number of vertices\n",
        "    \"\"\"\n",
        "    k = Y[:,0].max() + 1\n",
        "\n",
        "    Z = np.zeros((n,k))   \n",
        "    #nk: 1*n array, contains the number of observations in each class\n",
        "    #W: encoder marix. W[i,k] = {1/nk if Yi==k, otherwise 0}\n",
        "    nk = np.zeros((1,k))\n",
        "    W = np.zeros((n,k))\n",
        "\n",
        "    for i in range(k):\n",
        "      nk[0,i] = np.count_nonzero(Y[:,0]==i)\n",
        "\n",
        "    for i in range(Y.shape[0]):\n",
        "      k_i = Y[i,0]\n",
        "      if k_i >=0:\n",
        "        W[i,k_i] = 1/nk[0,k_i] ## GEE paper\n",
        "        # W[i,k_i] = nk[0,k_i]/n ## use as an example to show if people want to use nk/n instead of 1/nk\n",
        " \n",
        "    for row in X:\n",
        "      [v_i, v_j, edg_i_j] = row\n",
        "      v_i = int(v_i)\n",
        "      v_j = int(v_j)\n",
        "\n",
        "      label_i = Y[v_i][0] \n",
        "      label_j = Y[v_j][0]\n",
        "\n",
        "      if label_j >= 0:\n",
        "        Z[v_i, label_j] = Z[v_i, label_j] + W[v_j, label_j]*edg_i_j\n",
        "      if (label_i >= 0) and (v_i != v_j):\n",
        "        Z[v_j, label_i] = Z[v_j, label_i] + W[v_i, label_i]*edg_i_j\n",
        "\n",
        "    return Z, W\n",
        "\n",
        "  def Diagonal(self, X, n):\n",
        "    # add self-loop to edg list -- add 1 connection for each (i,i)\n",
        "    self_loops = np.column_stack((np.arange(n), np.arange(n), np.ones(n)))\n",
        "    # faster than vstack --  adding the second to the bottom\n",
        "    X = np.concatenate((X,self_loops), axis = 0)\n",
        "    return X\n",
        "\n",
        "  def Laplacian(self, X, n):\n",
        "    s = X.shape[0] # get the row number of the edg list\n",
        "\n",
        "    D = np.zeros((n,1))\n",
        "    for row in X:\n",
        "      [v_i, v_j, edg_i_j] = row\n",
        "      v_i = int(v_i)\n",
        "      v_j = int(v_j)\n",
        "      D[v_i] = D[v_i] + edg_i_j\n",
        "      if v_i != v_j:\n",
        "        D[v_j] = D[v_j] + edg_i_j\n",
        "\n",
        "    D = np.power(D, -0.5)\n",
        "    \n",
        "    for i in range(s):\n",
        "      X[i,2] = X[i,2] * D[int(X[i,0])] * D[int(X[i,1])]\n",
        "\n",
        "    return X\n",
        "  \n",
        "  def Correlation(self, Z):\n",
        "    \"\"\"\n",
        "      Calculate each row's 2-norm (Euclidean distance). \n",
        "      e.g.row_x: [ele_i,ele_j,ele_k]. norm2 = sqr(sum(ele_i^2+ele_i^2+ele_i^2))\n",
        "      then divide each element by their row norm\n",
        "      e.g. [ele_i/norm2,ele_j/norm2,ele_k/norm2]\n",
        "    \"\"\"\n",
        "    row_norm = LA.norm(Z, axis = 1)\n",
        "    reshape_row_norm = np.reshape(row_norm, (n,1))\n",
        "    Z = np.nan_to_num(Z/reshape_row_norm)\n",
        "\n",
        "    return Z\n",
        "    \n",
        "  def to_s3_list(self,X):\n",
        "    \"\"\"\n",
        "      if input X only has 2 columns, make it into s3 edge list.\n",
        "      this function will return a s3 edge list\n",
        "      [node_i, node_j, weight]...\n",
        "    \"\"\"\n",
        "    s = X.shape[0] # get the row number of the edg list\n",
        "    if X.shape[1] == 2:\n",
        "      # enlarge the edgelist to s*3 by adding 1 to the thrid position as adj(i,j)\n",
        "      X = np.insert(X, 1, np.ones(s,1))\n",
        "\n",
        "    return X\n",
        "\n",
        "############------------graph_encoder_embed_end------------------###############\n",
        "############------------Sparse_supervised_learning_start---------###############\n",
        "\n",
        "# https://www.kaggle.com/c/talkingdata-mobile-user-demographics/discussion/22567\n",
        "# https://github.com/tkipf/pygcn/blob/1600b5b748b3976413d1e307540ccc62605b4d6d/pygcn/utils.py#L73\n",
        "\n",
        "def batch_generator(X, y, k, batch_size, shuffle):\n",
        "    number_of_batches = int(X.shape[0]/batch_size)\n",
        "    counter = 0\n",
        "    sample_index = np.arange(X.shape[0])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "    while True:\n",
        "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
        "        X_batch = X[batch_index,:]\n",
        "        y_batch = to_categorical(y[batch_index], num_classes=k)\n",
        "        counter += 1\n",
        "        yield X_batch, y_batch\n",
        "        if (counter == number_of_batches):\n",
        "            if shuffle:\n",
        "                np.random.shuffle(sample_index)\n",
        "            counter = 0\n",
        "\n",
        "class Hyperperameters:\n",
        "  \"\"\"\n",
        "    define perameters for GNN.\n",
        "    default values are for GNN learning -- \"Leaner\" ==2:\n",
        "      embedding via partial label, then learn unknown label via two-layer NN\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # there is no scaled conjugate gradiant in keras optimiser, use defualt instead\n",
        "    # use whatever default\n",
        "    self.learning_rate = 0.01  # Initial learning rate.\n",
        "    self.epochs = 100 #Number of epochs to train.\n",
        "    self.hidden = 20 #Number of units in hidden layer \n",
        "    self.val_split = 0.1 #Split 10% of training data for validation\n",
        "    self.loss = 'categorical_crossentropy' # loss function\n",
        "\n",
        "class GNN:\n",
        "  def __init__(self, DataSets):\n",
        "    GNN.DataSets = DataSets\n",
        "    GNN.hyperM = Hyperperameters()\n",
        "    GNN.model = self.GNN_model()  #model summary: GNN.model.summary()\n",
        "      \n",
        " \n",
        "  def GNN_model(self):\n",
        "    \"\"\"\n",
        "      build GNN model\n",
        "    \"\"\"\n",
        "    hyperM = self.hyperM\n",
        "    DataSets = self.DataSets\n",
        "\n",
        "    z_train = DataSets.z_train\n",
        "    k = DataSets.d\n",
        "\n",
        "    feature_num = z_train.shape[1]\n",
        "    \n",
        "    model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape = (feature_num,)),  # input layer \n",
        "    keras.layers.Dense(hyperM.hidden, activation='relu'),  # hidden layer -- no tansig activation function in Keras, use relu instead\n",
        "    keras.layers.Dense(k, activation='softmax') # output layer, matlab used softmax for patternnet default ??? max(opts.neuron,K)? opts \n",
        "    ])\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = hyperM.learning_rate)\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=hyperM.loss,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "    \n",
        "  def GNN_run(self, flag):\n",
        "    \"\"\"\n",
        "      Train and test directly.\n",
        "      Do not learn from the unknown labels.\n",
        "    \"\"\"\n",
        "    gnn = copy.deepcopy(self)\n",
        "    hyperM = gnn.hyperM\n",
        "    DataSets = self.DataSets\n",
        "    k = DataSets.d\n",
        "    z_train = DataSets.z_train\n",
        "    y_train = DataSets.y_train\n",
        "    y_test = DataSets.y_test\n",
        "    z_test = DataSets.z_test\n",
        "    model = gnn.model    \n",
        "\n",
        "\n",
        "    if flag == \"direct\":\n",
        "      y_train_one_hot = to_categorical(y_train, num_classes=k)\n",
        "      train_strat = time.time() \n",
        "      history = model.fit(z_train, y_train_one_hot, \n",
        "        validation_split=hyperM.val_split,\n",
        "        epochs=hyperM.epochs, \n",
        "        shuffle=True,\n",
        "        verbose=0)\n",
        "    else:\n",
        "      early_stopping_callback = EarlyStopping(monitor='loss', patience=5, verbose=0)\n",
        "      checkpoint_callback = ModelCheckpoint('GNN.h5', monitor='loss', save_best_only=True, mode='min', verbose=0)\n",
        "      \n",
        "      train_strat = time.time()\n",
        "      history = model.fit(batch_generator(z_train, y_train, k, 32, True),\n",
        "                      epochs=hyperM.epochs,\n",
        "                      steps_per_epoch=z_train.shape[0],\n",
        "                      callbacks=[early_stopping_callback, checkpoint_callback],\n",
        "                      verbose=0)\n",
        "    train_end = time.time()\n",
        "    train_time = train_end - train_strat \n",
        "\n",
        "    y_test_one_hot = to_categorical(y_test, num_classes=k) \n",
        "    # set verbose to 0 to silent the output\n",
        "    test_loss, test_acc = gnn.model.evaluate(z_test,  y_test_one_hot, verbose=0) \n",
        "    return test_acc, train_time\n",
        "############------------Sparse_supervised_learning_end---------###############\n"
      ],
      "metadata": {
        "id": "d60qfGLDnY2q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-kF5bByQt4t"
      },
      "source": [
        "#Packages for Drive Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BjvPSUD9Qt4u"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "## for mount drive purpose\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0quiQZnHBMnE"
      },
      "source": [
        "#Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2MytS4OrlCCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea7eaa69-c6a1-4517-deb7-6ee831fcfbf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# mount drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "os.chdir('/content/drive/My Drive/Colab_Notebooks/Graph_ML/semi_dr.shen')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oIeHWJ519n_"
      },
      "source": [
        "# import ipynb packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z6GEpC02FnX",
        "outputId": "c9843b4a-2d2c-4a3a-8a05-1bd0ae2087d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import-ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.7.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (7.9.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->IPython->import-ipynb) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import-ipynb) (0.2.5)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.16.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.11.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->import-ipynb) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->import-ipynb) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.18.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n",
            "Installing collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4 jedi-0.18.1\n"
          ]
        }
      ],
      "source": [
        "!pip install import-ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jrl3lFk0gZM",
        "outputId": "9d75d3c3-4531-4fe6-ab4b-71f613628b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from test_cases.ipynb\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import import_ipynb\n",
        "from test_cases import Model, Case"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import import_ipynb\n",
        "from test_cases_numba import Model, Case"
      ],
      "metadata": {
        "id": "10_KZSd_8D2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d8ba0c-2882-4984-f70f-840b13eb6d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from test_cases_numba.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRMhWS392g7w"
      },
      "source": [
        "# Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############------------case_data_preprocess_start------------------############\n",
        "def adj_to_edg(A):\n",
        "  \"\"\"\n",
        "    input is the symmetric adjacency matrix: A\n",
        "    other variables in this function:\n",
        "    s: number of edges\n",
        "    return edg_list -- matrix format with shape(edg_sum,3):\n",
        "    example row in edg_list(matrix): [vertex1, vertex2, connection weight from Adj matrix]\n",
        "  \"\"\"\n",
        "  # check the len of the second dimenson of A\n",
        "  if A.shape[1] <= 3:\n",
        "    edg = A\n",
        "  else:\n",
        "    n = A.shape[0]\n",
        "    # construct the initial edgg_list matrix with the size of (edg_sum, 3)\n",
        "    edg_list = []\n",
        "    for i in range(n):\n",
        "      for j in range(i, n):\n",
        "        if A[i,j] > 0:\n",
        "          row = [i, j, A[i,j]]\n",
        "          edg_list.append(row)\n",
        "    edg = np.array(edg_list)\n",
        "  return edg\n",
        "############------------case_data_preprocess_end--------------------############"
      ],
      "metadata": {
        "id": "jg5RsEZDmzwm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case_original vs case_numba"
      ],
      "metadata": {
        "id": "V49yuvuWi3az"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHwAaAzhjJ_Q"
      },
      "outputs": [],
      "source": [
        "n = 5000\n",
        "case = Case(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Original-case"
      ],
      "metadata": {
        "id": "X_PEnk0zjJ_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "case_test = case.case_10_fully_known()\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8786c61e-fa8e-4e4f-b855-6c1899ef14b6",
        "id": "Vh8vKTjKjJ_R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89.68082427978516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Numba-case"
      ],
      "metadata": {
        "id": "KQtgJsuwjJ_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "case_test = case.case_10_fully_known()\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc06cbbc-6438-478a-be34-61ed2dc360a3",
        "id": "CQpJAwfBjJ_S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "62.95595097541809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GEE_Edge_list_Original vs GEE_Edge_list_Numba"
      ],
      "metadata": {
        "id": "5yl9LVMRjaxv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur9t5Xj3FWJ5"
      },
      "source": [
        "#### Case 10 with 3000 nodes (SBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BKkH7rdnuD-A"
      },
      "outputs": [],
      "source": [
        "n = 3000\n",
        "case = Case(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lVkPbp1uHNK",
        "outputId": "4abbc7d6-7206-49b1-ba1f-65369cf58092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info:\n",
            "\n",
            "    SBM with 3 classes and defined probabilities with fully known labels\n",
            "    80% for training and 20% for testing\n",
            "    \n",
            "n:\n",
            "<class 'int'>\n",
            "3000\n",
            "d:\n",
            "<class 'int'>\n",
            "3\n",
            "X:\n",
            "(3000, 3000)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 1 0]]\n",
            "Y:\n",
            "(3000, 1)\n",
            "[[1]\n",
            " [0]\n",
            " [2]\n",
            " ...\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "case_test = case.case_10_fully_known()\n",
        "case_test.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_list = adj_to_edg(case_test.X)"
      ],
      "metadata": {
        "id": "fjsnCBBynXZj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEB2KhBIIbD9"
      },
      "source": [
        "#### Laplacian = True, DiagA = False, Correlation = False -- original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c45e68-83bd-4c3a-bda5-2a537a38d0dd",
        "id": "EoMMNM6UIbD9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- embed 7.517747640609741 seconds ---\n"
          ]
        }
      ],
      "source": [
        "GEE = GraphEncoderEmbed()\n",
        "Z, W, emb_time = GEE.run(edge_list, case_test.Y, case_test.n, Laplacian = True, DiagA = False, Correlation = False)\n",
        "print(\"--- embed %s seconds ---\" % emb_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQQ4cDfkh1xu"
      },
      "source": [
        "#### Laplacian = True, DiagA = False, Correlation = False -- Numba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74334e9-6d9e-4ce8-93c5-1790c3b567bc",
        "id": "wDZiX0pVh1xu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- embed 7.582218408584595 seconds ---\n"
          ]
        }
      ],
      "source": [
        "Z, W, emb_time = GEE_run(case_test.Y, case_test.n, edge_list, Laplacian = True, DiagA = False, Correlation = False)\n",
        "print(\"--- embed %s seconds ---\" % emb_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Laplacian = True, DiagA = True, Correlation = True -- original"
      ],
      "metadata": {
        "id": "NIWszloZoPz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEE = GraphEncoderEmbed()\n",
        "Z, W, emb_time = GEE.run(edge_list, case_test.Y, case_test.n, Laplacian = True, DiagA = True, Correlation = True)\n",
        "print(\"--- embed %s seconds ---\" % emb_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOKr50DmoLip",
        "outputId": "0553cc26-0c78-48ba-dbb2-13da0fe92172"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- embed 12.64570951461792 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Laplacian = True, DiagA = True, Correlation = True -- Numba"
      ],
      "metadata": {
        "id": "CZViA2tMojM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z, W, emb_time = GEE_run(case_test.Y, case_test.n, edge_list, Laplacian = True, DiagA = True, Correlation = True)\n",
        "print(\"--- embed %s seconds ---\" % emb_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWX1OXW4onPa",
        "outputId": "206cedd4-8653-4bfc-bbcb-d36d5a172533"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- embed 7.053375482559204 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtRTsYmHLRKR"
      },
      "source": [
        "### Case 10 with 5000 nodes (SBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ig_jPQULRKS"
      },
      "outputs": [],
      "source": [
        "n = 5000\n",
        "case = Case(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY_ZL6PVLRKS",
        "outputId": "d6cea719-c8c9-4665-8959-eb178bda1eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info:\n",
            "\n",
            "    SBM with 3 classes and defined probabilities with fully known labels\n",
            "    80% for training and 20% for testing\n",
            "    \n",
            "n:\n",
            "<class 'int'>\n",
            "5000\n",
            "d:\n",
            "<class 'int'>\n",
            "3\n",
            "X:\n",
            "(5000, 5000)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Y:\n",
            "(5000, 1)\n",
            "[[1]\n",
            " [0]\n",
            " [2]\n",
            " ...\n",
            " [2]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "case_test = case.case_10_fully_known()\n",
        "case_test.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_list = adj_to_edg(case_test.X)"
      ],
      "metadata": {
        "id": "2pJhMiOP2l73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Laplacian = True, DiagA = True, Correlation = True"
      ],
      "metadata": {
        "id": "F_c7Z6Fv0VyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Numba"
      ],
      "metadata": {
        "id": "WwzxcHqkk1yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z, W, emb_time = GEE_run(case_test.Y, case_test.n, edge_list, Laplacian = True, DiagA = True, Correlation = True)\n",
        "print(\"--- embed %s seconds ---\" % emb_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biuoMVVJ7je7",
        "outputId": "af1e50b5-1b40-47c0-9a7b-46e3ad4c0bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- embed 16.475561141967773 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Original"
      ],
      "metadata": {
        "id": "5_rnVVdBk9C1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEE = GraphEncoderEmbed()\n",
        "Z, W, emb_time = GEE.run(edge_list, case_test.Y, case_test.n, Laplacian = True, DiagA = True, Correlation = True)\n",
        "print(\"--- embed %s seconds ---\" % emb_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE6O7PqL0UxJ",
        "outputId": "e6c8c67f-08fe-4f87-c6f7-8b6d4ea74f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- embed 19.47906732559204 seconds ---\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3TsGU1EpgORN",
        "4N9L480srX8a",
        "V49yuvuWi3az",
        "X_PEnk0zjJ_R",
        "KQtgJsuwjJ_S",
        "gEB2KhBIIbD9",
        "IQQ4cDfkh1xu",
        "NIWszloZoPz8",
        "CZViA2tMojM2",
        "rtRTsYmHLRKR",
        "WwzxcHqkk1yi",
        "5_rnVVdBk9C1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}