{"cells":[{"cell_type":"markdown","metadata":{"id":"YdFl3x2SBBFf"},"source":["#Package Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1feGDRiBv-2-"},"outputs":[],"source":["import sys\n","import numpy as np\n","import copy\n","from numpy import linalg as LA\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics.cluster import adjusted_rand_score\n","from sklearn.cluster import KMeans\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn import metrics\n","import time\n","# node2vec\n","# from node2vec import Node2Vec\n","import networkx as nx\n","# for sparse matrix\n","from scipy import sparse\n","#early stop\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.callbacks import ModelCheckpoint"]},{"cell_type":"markdown","metadata":{"id":"4N9L480srX8a"},"source":["#Classes and functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpkZjKLAskam"},"outputs":[],"source":["# Supress/hide the warning\n","# invalide devide resutls will be nan\n","np.seterr(divide='ignore', invalid='ignore')\n","\n","############------------Auto_select_method_start-----------------###############\n","def Run(case, learn_opt, **kwargs):\n","  \"\"\"\n","    input X can be a list of one of these format below:\n","    1. python list of n*n adjacency matrices.\n","    2. python list of s*2 edge lists.\n","    3. python list of s*3 edge lists.\n","    input Y can be these choices below:\n","    1. no Y input. The default will be [2,3,4,5] -- K range for clusters.\n","    2. n*1 class -- label vector. Positive labels are knwon labels and -1 indicate unknown labels.\n","    3. A range of potential number of clusters -- K (K clusters in total), i.e., [3, 4, 5].\n","\n","    if input X is n*n adjacency =>  s*3 edg list\n","    if input X is s*2 => s*3 edg list\n","\n","    Vertex size should be >10.\n","\n","    Clustering / Classification\n","    The program automaticlly decide to run clustering or classification.\n","    1. If Y is a given cluster range, do clustering (case 1,3 for Y).\n","    2. If Y is a label vector (case 2 for Y), do classification.\n","    For classification: semi-supervised learning, supervised learning methods.\n","                        see the \"Learner\" defined below.\n","\n","\n","    Supervised learning \"Learner\":\n","      **Note the input trining set (X) need has fully known labels in Y.\n","      Learner = 1 run LDA, test on test set\n","      Learner = 0 run NN, test on test set\n","\n","    Semi-supervised learning \"Learner\":\n","       **Note the input trining set (X) need some unknown label(s) in Y.\n","      Learner=0 means embedding via known label, do not learn the unknown labels.\n","           Since only some nodes in the training set has known label,\n","           the test set is the unknwon labeled set, which is compared with\n","           the original labels of the unknown set\n","      Learner=1 means embedding via partial label, then learn unknown label via LDA.\n","        this runs semi-supervised learning with NN,\n","        the test will be on the result labels with the original labels\n","\n","      Learner=2 means embedding via partial label, then learn unknown label via two-layer NN.\n","        this runs semi-supervised learning with NN,\n","        the test will be on the result labels with the original labels\n","\n","\n","  \"\"\"\n","  defaultKwargs = {'Y':[2,3,4,5], 'DiagA': True,'Correlation': True,'Laplacian': False,\n","                  'Learner': 1, 'LearnerIter': 0, 'MaxIter': 50, 'MaxIterK': 5,\n","                  'Replicates': 3, 'Attributes': False, 'neuron': 20, 'activation': 'relu',\n","                   'emb_opt': 'AEE', 'sparse_opt': 'None', 'Batch_input': False}\n","  kwargs = { **defaultKwargs, **kwargs }\n","  train_time = None # no seperate training time for semisuperviised learning yet\n","  total_begin = time.time()\n","  eval = Evaluation()\n","  kwargs_for_DataPreprocess =  {k: kwargs[k] for k in ['DiagA', 'Laplacian', 'Correlation', 'Attributes', 'emb_opt']}\n","  Dataset = DataPreprocess(case, **kwargs_for_DataPreprocess)\n","\n","  Y = case.Y\n","  n = case.n\n","\n","  # auto check block\n","  # if the option is not clustering, but the Y does not contain labels (known/unknwon) for n nodes.\n","  if (learn_opt != \"c\") and (len(Y) != n):\n","    learn_opt = \"c\" # do clustering\n","    print(\"The given Y do not have the same size as the node.Y is assumed as cluster number range.\",\n","    \"Clustering will be performed.\",\n","    \"If you want to do classification, stop the current run, reimport the Y with the right format then run again.\",\n","    sep = \"\\n\")\n","\n","  # clustering\n","  if learn_opt == 'c':\n","    cluster = Clustering(Dataset)\n","    Z, Y, W, meanSS = cluster.cluster_main()\n","    if case.Y_ori is not None:\n","      ari = eval.clustering_test(Y, case.Y_ori)\n","      print(\"ARI: \", ari)\n","    else:\n","      ari = None\n","\n","  # supervised learning\n","  if learn_opt == \"su\":\n","    Dataset = Dataset.supervise_preprocess()\n","    kwargs_for_learner = {k: kwargs[k] for k in ['Learner', 'LearnerIter', 'Batch_input']}\n","    train_strat = time.time()\n","    if kwargs['Learner'] == 1:\n","      lda = LDA(Dataset, **kwargs_for_learner)\n","      lda_res = lda.LDA_Learner(lda.DataSets)\n","      acc = eval.LDA_supervise_test(lda_res, Dataset.z_test, Dataset.Y_test)\n","    if kwargs['Learner'] == 0:\n","      gnn = GNN(Dataset, **kwargs_for_learner)\n","      gnn_res = gnn.GNN_complete()\n","      acc = eval.GNN_supervise_test(gnn_res, Dataset.z_test, Dataset.Y_test)\n","    train_end = time.time()\n","    train_time = train_end - train_strat\n","    print(\"acc: \", acc)\n","\n","  # semisupervised learning\n","  if learn_opt == \"se\":\n","    Dataset = Dataset.semi_supervise_preprocess()\n","    kwargs_for_learner = {k: kwargs[k] for k in ['Learner', 'LearnerIter', 'Batch_input']}\n","    if kwargs['Learner'] == 2:\n","      gnn = GNN(Dataset, **kwargs_for_learner)\n","      gnn_res = gnn.GNN_complete()\n","      acc = eval.GNN_semi_supervised_learn_test(gnn_res.Y, case.Y_ori)\n","    if kwargs['Learner'] == 1:\n","      lda = LDA(Dataset, **kwargs_for_learner)\n","      lda_res = lda.LDA_Iter()\n","      acc = eval.GNN_semi_supervised_learn_test(lda_res.Y, case.Y_ori)\n","    if kwargs['Learner'] == 0:\n","      gnn = GNN(Dataset, **kwargs_for_learner)\n","      gnn_res = gnn.GNN_complete()\n","      acc = eval.GNN_semi_supervised_not_learn_test(gnn_res, Dataset, case)\n","    print(\"acc: \", acc)\n","\n","  total_end = time.time()\n","  emb_time = Dataset.embed_time\n","  total_time = total_end - total_begin\n","  print(\"--- embed %s seconds ---\" % emb_time)\n","  if train_time:\n","    print(\"--- train %s seconds ---\" % train_time)\n","  print(\"--- total %s seconds ---\" % total_time)\n","\n","  if learn_opt != \"c\":\n","    Z_ori = Dataset.Z\n","    W_ori = Dataset.W\n","\n","    sparse_opt = kwargs['sparse_opt']\n","    Z = To_single_sparse_matrix(Dataset.Z, sparse_opt)\n","    W = To_multi_sparse_matrix(Dataset.W, sparse_opt)\n","\n","    return acc, train_time, emb_time, total_time, Z, W, Z_ori, W_ori\n","\n","  else:\n","    return ari, train_time, emb_time, total_time, Z, W, Y\n","\n","############------------node2vec_embed_start--------------------################\n","def node2vec_embed(X):\n","  G = nx.from_numpy_matrix(X)\n","  # use default setting from https://github.com/eliorc/node2vec\n","  node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4)\n","  # Embed nodes, use default setting from https://github.com/eliorc/node2vec\n","  model = node2vec.fit(window=2, min_count=1, batch_words=4)\n","  # get embedding matrix\n","  Z = model.wv.vectors\n","  return Z\n","\n","############------------node2vec_embed_end----------------------################\n","############------------graph_encoder_embed_start----------------###############\n","def graph_encoder_embed(X,Y,n,**kwargs):\n","  \"\"\"\n","    input X is s*3 edg list: nodei, nodej, connection weight(i,j)\n","    graph embedding function\n","  \"\"\"\n","  defaultKwargs = {'Correlation': True}\n","  kwargs = { **defaultKwargs, **kwargs}\n","\n","  #If Y has more than one dimention , Y is the range of cluster size for a vertex. e.g. [2,10], [2,5,6]\n","  # check if Y is the possibility version. e.g.Y: n*k each row list the possibility for each class[0.9, 0.1, 0, ......]\n","  possibility_detected = False\n","  if Y.shape[1] > 1:\n","    k = Y.shape[1]\n","    possibility_detected = True\n","  else:\n","    # assign k to the max along the first column\n","    # Note for python, label Y starts from 0. Python index starts from 0. thus size k should be max + 1\n","    k = Y[:,0].max() + 1\n","\n","  #nk: 1*n array, contains the number of observations in each class\n","  #W: encoder marix. W[i,k] = {1/nk if Yi==k, otherwise 0}\n","  nk = np.zeros((1,k))\n","  W = np.zeros((n,k))\n","\n","  if possibility_detected:\n","    # sum Y (each row of Y is a vector of posibility for each class), then do element divid nk.\n","    nk=np.sum(Y, axis=0)\n","    W=Y/nk\n","  else:\n","    for i in range(k):\n","      nk[0,i] = np.count_nonzero(Y[:,0]==i)\n","\n","    for i in range(Y.shape[0]):\n","      k_i = Y[i,0]\n","      if k_i >=0:\n","        W[i,k_i] = 1/nk[0,k_i]\n","\n","\n","  # Edge List Version in O(s)\n","  Z = np.zeros((n,k))\n","  i = 0\n","  for row in X:\n","    [v_i, v_j, edg_i_j] = row\n","    v_i = int(v_i)\n","    v_j = int(v_j)\n","    if possibility_detected:\n","      for label_j in range(k):\n","        Z[v_i, label_j] = Z[v_i, label_j] + W[v_j, label_j]*edg_i_j\n","        if v_i != v_j:\n","          Z[v_j, label_j] = Z[v_j, label_j] + W[v_i, label_j]*edg_i_j\n","    else:\n","      label_i = Y[v_i][0]\n","      label_j = Y[v_j][0]\n","\n","      if label_j >= 0:\n","        Z[v_i, label_j] = Z[v_i, label_j] + W[v_j, label_j]*edg_i_j\n","      if (label_i >= 0) and (v_i != v_j):\n","        Z[v_j, label_i] = Z[v_j, label_i] + W[v_i, label_i]*edg_i_j\n","\n","  # Calculate each row's 2-norm (Euclidean distance).\n","  # e.g.row_x: [ele_i,ele_j,ele_k]. norm2 = sqr(sum(ele_i^2+ele_i^2+ele_i^2))\n","  # then divide each element by their row norm\n","  # e.g. [ele_i/norm2,ele_j/norm2,ele_k/norm2]\n","  if kwargs['Correlation']:\n","    row_norm = LA.norm(Z, axis = 1)\n","    reshape_row_norm = np.reshape(row_norm, (n,1))\n","    Z = np.nan_to_num(Z/reshape_row_norm)\n","\n","  return Z, W\n","\n","\n","def multi_graph_encoder_embed(DataSets, Y, **kwargs):\n","  \"\"\"\n","    input X contains a list of s3 edge list\n","    get Z and W by using graph emcode embedding\n","    Z is the concatenated embedding matrix from multiple graphs\n","    if there are attirbutes provided, add attributes to Z\n","    W is a list of weight matrix Wi\n","  \"\"\"\n","  kwargs_single = {**kwargs}\n","\n","  X = DataSets.X\n","  n = DataSets.n\n","  U = DataSets.U\n","  Graph_count = DataSets.Graph_count\n","  attributes = DataSets.attributes\n","  kwargs = DataSets.kwargs\n","\n","  W = []\n","\n","  for i in range(Graph_count):\n","    if i == 0:\n","      [Z, Wi] = graph_encoder_embed(X[i],Y,n,**kwargs_single)\n","    else:\n","      [Z_new, Wi] = graph_encoder_embed(X[i],Y,n,**kwargs)\n","      Z = np.concatenate((Z, Z_new), axis=1)\n","    W.append(Wi)\n","\n","  # if there is attributes matrix U provided, add U\n","  if attributes:\n","    # add U to Z side by side\n","    Z = np.concatenate((Z, U), axis=1)\n","\n","  return Z, W\n","\n","############------------graph_encoder_embed_end------------------###############\n","\n","############------------DataPreprocess_start---------------------###############\n","class DataPreprocess:\n","  def __init__(self, Dataset_input, **kwargs):\n","    self.kwargs = self.kwargs_construct(**kwargs)\n","    # Note, since every element in multi-graph list X has the same size and\n","    # node index, there will be only one column in Y for the node labels\n","    self.Y = Dataset_input.Y\n","    self.n = Dataset_input.n\n","    (self.X, self.Graph_count, self.embed_time) = self.input_prep(Dataset_input.X)\n","    (self.attributes, self.U) = self.check_attributes()\n","    self.Dataset_input = Dataset_input\n","\n","\n","  def kwargs_construct(self, **kwargs):\n","    defaultKwargs = {'DiagA': True,'Laplacian': False,  #input_prep\n","                     'Correlation': True,      # graph_encoder_embed\n","                     'Attributes': False,      # GNN_preprocess\n","                     }\n","    kwargs = { **defaultKwargs, **kwargs}  # update the args using input_args\n","    return kwargs\n","\n","\n","  def check_attributes(self):\n","    \"\"\"\n","      return attributes detected flag and attributes U\n","    \"\"\"\n","    kwargs = self.kwargs\n","\n","    Attributes_detected = False\n","    U = None\n","\n","    if kwargs[\"Attributes\"]:\n","      U = kwargs[\"Attributes\"]\n","      if U.shape[0] == n:\n","        Attributes_detected = True\n","      else:\n","        print(\"Attributes need to have the same size as the nodes.\\\n","        If n nodes, need n rows\")\n","    return Attributes_detected, U\n","\n","\n","  def test_edg_list_to_adj(self, n_test, n, edg_list):\n","    adj = np.zeros((n_test,n))\n","\n","    for row in edg_list:\n","      [node_i, node_j, edge_i_j] = row\n","      adj[node_i, node_j] = edge_i_j\n","\n","    return adj\n","\n","\n","  def input_prep(self, X):\n","    kwargs = self.kwargs\n","    # if X is a single numpy object, put this numpy object in a list\n","    if type(X) == np.ndarray:\n","      X = [X]\n","\n","    ## Now X is a list of numpy objects\n","    # each element can be a numpy object for adjacency matrix or edge list\n","    Graph_count = len(X)\n","\n","    # AEE needs X to be a list of edge list\n","    if kwargs[\"emb_opt\"] == \"AEE\":\n","      X, embed_time = self.input_prep_AEE(X, Graph_count)\n","    # Node2Vec only needs a list of adjacency matrix\n","    if kwargs[\"emb_opt\"] == \"Node2Vec\":\n","      embed_time = 0\n","      pass\n","\n","    return X, Graph_count, embed_time\n","\n","\n","  def input_prep_AEE(self, X, Graph_count):\n","    \"\"\"\n","      X may be a single numpy object or a list of numpy objects\n","      The multi-graph input X is assumed has the same node numbers\n","      for each element in X, and the node are indexed the same way\n","      amonge the elements. e.g. node_0 in X[1] is the same node_0 in X[2].\n","      return X as a list of s*3 edge lists\n","      return n, which is the total number of nodes\n","    \"\"\"\n","\n","    # need total labeled number n\n","    # if try to get from the edg list, it may miss the node that has no connection with others but has label\n","    n = self.n\n","\n","    embed_time = 0\n","    for i in range(Graph_count):\n","      X_tmp = X[i]\n","      X_tmp = self.to_s3_list(X_tmp)\n","\n","      # count the time for laplacian and diagnal only\n","      embed_begin = time.time()\n","      X_tmp = self.single_X_prep(X_tmp, n)\n","      embed_end = time.time()\n","      embed_time += (embed_end - embed_begin)\n","\n","      X[i] = X_tmp\n","\n","    return X, embed_time\n","\n","\n","  def to_s3_list(self,X):\n","    \"\"\"\n","      the input X is a signle graph, can be adjacency matrix or edgelist\n","      this function will return a s3 edge list\n","    \"\"\"\n","    (s,t) = X.shape\n","\n","    if s == t:\n","      # convert adjacency matrix to edgelist\n","      X = self.adj_to_edg(X);\n","    else:\n","      # for either s*2 or s*3 case, calculate n -- vertex number\n","      if t == 2:\n","        # enlarge the edgelist to s*3 by adding 1 to the thrid position as adj(i,j)\n","        X = np.insert(X, 1, np.ones(s,1))\n","\n","    return X\n","\n","\n","  def single_X_prep(self, X, n):\n","    \"\"\"\n","      input X is a single S3 edge list\n","      this adds Diagnal augement and Laplacian normalization to the edge list\n","    \"\"\"\n","    kwargs = self.kwargs\n","\n","    X = X.astype(np.float32)\n","\n","    # Diagnal augment\n","    if kwargs['DiagA']:\n","      # add self-loop to edg list -- add 1 connection for each (i,i)\n","      self_loops = np.column_stack((np.arange(n), np.arange(n), np.ones(n)))\n","      # faster than vstack --  adding the second to the bottom\n","      X = np.concatenate((X,self_loops), axis = 0)\n","\n","    # Laplacian\n","    s = X.shape[0] # get the row number of the edg list\n","    if kwargs[\"Laplacian\"]:\n","      D = np.zeros((n,1))\n","      for row in X:\n","        [v_i, v_j, edg_i_j] = row\n","        v_i = int(v_i)\n","        v_j = int(v_j)\n","        D[v_i] = D[v_i] + edg_i_j\n","        if v_i != v_j:\n","          D[v_j] = D[v_j] + edg_i_j\n","\n","      D = np.power(D, -0.5)\n","\n","      for i in range(s):\n","        X[i,2] = X[i,2] * D[int(X[i,0])] * D[int(X[i,1])]\n","\n","    return X\n","\n","\n","  def adj_to_edg(self,A):\n","    \"\"\"\n","      input is the symmetric adjacency matrix: A\n","      other variables in this function:\n","      s: number of edges\n","      return edg_list -- matrix format with shape(edg_sum,3):\n","      example row in edg_list(matrix): [vertex1, vertex2, connection weight from Adj matrix]\n","    \"\"\"\n","    # check the len of the second dimenson of A\n","    if A.shape[1] <= 3:\n","      edg = A\n","    else:\n","      n = A.shape[0]\n","      # construct the initial edgg_list matrix with the size of (edg_sum, 3)\n","      edg_list = []\n","      for i in range(n):\n","        for j in range(i, n):\n","          if A[i,j] > 0:\n","            row = [i, j, A[i,j]]\n","            edg_list.append(row)\n","      edg = np.array(edg_list)\n","    return edg\n","\n","\n","  def semi_supervise_preprocess(self):\n","    \"\"\"\n","      get Z, W using multi_graph_encoder_embed()\n","      get training sets and testing sets for Z and Y by using split_data()\n","\n","    \"\"\"\n","    DataSets =  copy.deepcopy(self)\n","    Y = DataSets.Y\n","    kwargs = DataSets.kwargs\n","    Encoder_kwargs = {k: kwargs[k] for k in ['Correlation']}\n","    # semisupervise do embedding during the learning process\n","    # this timer is only for the first embedding for the normalized input X\n","    embed_time_main_begin = time.time()\n","    if kwargs[\"emb_opt\"] == \"AEE\":\n","      (DataSets.Z, DataSets.W) = multi_graph_encoder_embed(DataSets, Y, **Encoder_kwargs)\n","    if kwargs[\"emb_opt\"] == \"Node2Vec\":\n","      DataSets.Z = node2vec_embed(DataSets.X)\n","\n","    embed_time_main_end = time.time()\n","    embed_time_main = embed_time_main_end - embed_time_main_begin\n","\n","    DataSets.k = DataSets.get_k()\n","    DataSets = DataSets.split_data()\n","    DataSets.embed_time = DataSets.embed_time + embed_time_main\n","\n","    return DataSets\n","\n","\n","  def get_k(self):\n","    Y = self.Y\n","    n = self.n\n","    # get class number k or the largest cluster size\n","    # max of all flattened element + 1\n","    if len(Y) == n:\n","      k = np.amax(Y) + 1\n","    return k\n","\n","\n","  def split_data(self):\n","    split_Sets =  copy.deepcopy(self)\n","\n","    Y = split_Sets.Y\n","    Z = split_Sets.Z\n","\n","    ind_train = np.argwhere (Y >= 0)[:,0]\n","    ind_unlabel = np.argwhere (Y < 0)[:,0]\n","\n","    Y_train = Y[ind_train, 0]\n","    z_train = Z[ind_train]\n","\n","    Y_unlabel = None\n","    z_unlabel = None\n","\n","    if len(ind_unlabel) > 0:\n","      Y_unlabel = Y[ind_unlabel, 0]\n","      z_unlabel = Z[ind_unlabel]\n","\n","    # Convert targets into one-hot encoded format\n","    Y_train_one_hot = to_categorical(Y_train)\n","\n","    split_Sets.ind_unlabel = ind_unlabel\n","    split_Sets.ind_train = ind_train\n","    split_Sets.Y_train = Y_train\n","    split_Sets.Y_unlabel = Y_unlabel\n","    split_Sets.z_train = z_train\n","    split_Sets.z_unlabel = z_unlabel\n","    split_Sets.Y_train_one_hot = Y_train_one_hot\n","\n","    return split_Sets\n","\n","\n","  def DataSets_reset(self, option):\n","    \"\"\"\n","      based on the information of the given new Y:\n","      1. reassign Z and W to the given DataSets,\n","      2. update z_train, z_unlabel\n","      Input Option:\n","      1. if the option is \"y_temp\", do graph encoder using y_temp\n","    \"\"\"\n","    NewSets =  copy.deepcopy(self)\n","    kwargs = NewSets.kwargs\n","    ind_unlabel = NewSets.ind_unlabel\n","    ind_train = NewSets.ind_train\n","    y_temp =  NewSets.y_temp\n","    Encoder_kwargs = {k: kwargs[k] for k in ['Correlation']}\n","\n","    # different versions\n","    if option == \"y_temp\":\n","      [Z,W] = multi_graph_encoder_embed(NewSets, y_temp, **Encoder_kwargs)\n","    if option == \"y_temp_one_hot\":\n","      y_temp_one_hot = NewSets.y_temp_one_hot\n","      [Z,W] = multi_graph_encoder_embed(NewSets, y_temp_one_hot, **Encoder_kwargs)\n","    if NewSets.attributes:\n","      # add U to Z side by side\n","      Z = np.concatenate((Z, NewSets.U), axis=1)\n","\n","    NewSets.Z = Z\n","    NewSets.W = W\n","    NewSets.z_train = Z[ind_train]\n","    NewSets.z_unlabel = Z[ind_unlabel]\n","\n","    return NewSets\n","\n","\n","  def supervise_preprocess(self):\n","    \"\"\"\n","      adding test sets for supervised learning\n","      this function assumes only one test set\n","      if there is a list of test set, needs to modify this function\n","    \"\"\"\n","\n","    DataSets = self.semi_supervise_preprocess()\n","    Dataset_input = DataSets.Dataset_input\n","\n","    DataSets.z_test = DataSets.Z[Dataset_input.test_idx]\n","    DataSets.Y_test = Dataset_input.Y_test.ravel()\n","    DataSets.z_unlabel = None\n","    DataSets.Y_unlabel = None\n","\n","    return DataSets\n","############------------DataPreprocess_end-----------------------###############\n","\n","############-----------------GNN_start---------------------------###############\n","def batch_generator(X, y, k, batch_size, shuffle):\n","    number_of_batches = int(X.shape[0]/batch_size)\n","    counter = 0\n","    sample_index = np.arange(X.shape[0])\n","    if shuffle:\n","        np.random.shuffle(sample_index)\n","    while True:\n","        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n","        X_batch = X[batch_index,:]\n","        y_batch = y[batch_index,:]\n","        counter += 1\n","        yield X_batch, y_batch\n","        if (counter == number_of_batches):\n","            if shuffle:\n","                np.random.shuffle(sample_index)\n","            counter = 0\n","\n","class Hyperperameters:\n","  \"\"\"\n","    define perameters for GNN.\n","    default values are for GNN learning -- \"Leaner\" ==2:\n","      embedding via partial label, then learn unknown label via two-layer NN\n","\n","  \"\"\"\n","  def __init__(self):\n","    # there is no scaled conjugate gradiant in keras optimiser, use defualt instead\n","    # use whatever default\n","    self.learning_rate = 0.01  # Initial learning rate.\n","    self.epochs = 100 #Number of epochs to train.\n","    self.hidden = 20 #Number of units in hidden layer\n","    self.val_split = 0.1 #Split 10% of training data for validation\n","    self.loss = 'categorical_crossentropy' # loss function\n","\n","\n","class GNN:\n","  def __init__(self, DataSets, **kwargs):\n","    GNN.kwargs = self.kwargs_construct(**kwargs)\n","    GNN.DataSets = DataSets\n","    GNN.hyperM = Hyperperameters()\n","    GNN.model = self.GNN_model()  #model summary: GNN.model.summary()\n","    GNN.meanSS = 0  # initialize the self-defined critirion meanSS\n","\n","  def kwargs_construct(self, **kwargs):\n","    defaultKwargs = {'Learner': 1,                    # GNN_Leaner\n","                     'LearnerIter': 0,                # GNN_complete, GNN_Iter\n","                     \"Replicates\": 3,                 # GNN_Iter\n","                     \"Batch_input\": False              # if run in batches\n","                     }\n","    kwargs = { **defaultKwargs, **kwargs}  # update the args using input_args\n","    return kwargs\n","\n","\n","  def GNN_model(self):\n","    \"\"\"\n","      build GNN model\n","    \"\"\"\n","    hyperM = self.hyperM\n","    DataSets = self.DataSets\n","\n","    z_train = DataSets.z_train\n","    k = DataSets.k\n","\n","    feature_num = z_train.shape[1]\n","\n","    model = keras.Sequential([\n","    keras.layers.Flatten(input_shape = (feature_num,)),  # input layer\n","    keras.layers.Dense(hyperM.hidden, activation='relu'),  # hidden layer -- no tansig activation function in Keras, use relu instead\n","    keras.layers.Dense(k, activation='softmax') # output layer, matlab used softmax for patternnet default ??? max(opts.neuron,K)? opts\n","    ])\n","\n","    optimizer = keras.optimizers.Adam(learning_rate = hyperM.learning_rate)\n","\n","    model.compile(optimizer='adam',\n","                  loss=hyperM.loss,\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","\n","  def GNN_run(self, k, z_train, y_train_one_hot, z_unlabel):\n","    \"\"\"\n","      Train and test directly.\n","      Do not learn from the unknown labels.\n","    \"\"\"\n","    gnn = copy.deepcopy(self)\n","    hyperM = gnn.hyperM\n","    model = gnn.model\n","    batch_flag = self.kwargs[\"Batch_input\"]\n","\n","    if batch_flag:\n","      early_stopping_callback = EarlyStopping(monitor='loss', patience=5, verbose=0)\n","      checkpoint_callback = ModelCheckpoint('GNN.h5', monitor='loss', save_best_only=True, mode='min', verbose=0)\n","\n","      history = model.fit(batch_generator(z_train, y_train_one_hot, k, 32, True),\n","                      epochs=hyperM.epochs,\n","                      steps_per_epoch=z_train.shape[0],\n","                      callbacks=[early_stopping_callback, checkpoint_callback],\n","                      verbose=0)\n","    else:\n","      # validation_split=hyperM.val_split\n","      history = model.fit(z_train, y_train_one_hot,\n","            validation_split=hyperM.val_split,\n","            epochs=hyperM.epochs,\n","            shuffle=True,\n","            verbose=0)\n","\n","    train_acc = history.history['accuracy'][-1]\n","\n","    predict_probs = None\n","    pred_class = None\n","    pred_class_prob = None\n","    if type(z_unlabel) == np.ndarray:\n","      # predict_probas include probabilities for all classes for each node\n","      predict_probs = model.predict(z_unlabel)\n","      # assign the classes with the highest probability\n","      pred_class = np.argmax(predict_probs, axis=1)\n","      # the corresponding probabilities of the predicted classes\n","      pred_class_prob = predict_probs[range(len(predict_probs)),pred_class]\n","\n","    gnn.model = model\n","    gnn.train_acc = train_acc\n","    gnn.pred_probs = predict_probs\n","    gnn.pred_class = pred_class\n","    gnn.pred_class_prob = pred_class_prob\n","\n","\n","    return gnn\n","\n","  def GNN_Direct(self, DataSets, y_train_one_hot):\n","    \"\"\"\n","      This function can run:\n","      1. by itself, when interation is set to False (<1)\n","      2. inside GNN_Iter, when interation is set to True (>=1)\n","\n","      Learner == 0: GNN, but not learn from the known label\n","      Learner == 2: GNN, and learn unknown labels\n","    \"\"\"\n","    Learner = self.kwargs[\"Learner\"]\n","\n","    k = DataSets.k\n","    z_train = DataSets.z_train\n","    Y = DataSets.Y\n","    z_unlabel = DataSets.z_unlabel\n","    ind_unlabel = DataSets.ind_unlabel\n","\n","    gnn = self.GNN_run(k, z_train, y_train_one_hot, z_unlabel)\n","\n","    if Learner == 0:\n","      # do not learn unknown label.\n","      pass\n","\n","    if Learner == 2:\n","      # learn unknown label based on the known label\n","      # replace the unknown label in Y with predicted labels\n","      pred_class = gnn.pred_class\n","      Y[ind_unlabel, 0] = pred_class\n","\n","    gnn.Y = Y\n","\n","    return gnn\n","\n","\n","  def GNN_Iter(self, DataSets):\n","    \"\"\"\n","      Run this function when interation is set, which is >=1.\n","\n","      1. randomly assign labels to the unknown labels, get Y_temp\n","      2. get Y_one_hot for the Y_temp\n","      3. get Z from graph_encod function with X and Y_temp\n","      within each loop:\n","        use meanSS as its criterion to decide if the update is needed\n","\t      update Y_one_hot for the unknown labels with predict probabilities of each classes\n","\t      update Y with the highest possible predicted labels\n","\t      update z_train and z_unlabel from graph encoder embedding using the updated Y\n","\t      train the model with the updated z_train and Y_one_hot\n","    \"\"\"\n","\n","    kwargs = self.kwargs\n","    meanSS = self.meanSS\n","\n","    k = DataSets.k\n","    Y = DataSets.Y\n","    ind_unlabel = DataSets.ind_unlabel\n","\n","\n","    y_temp = np.copy(Y)\n","    DataSets.y_temp = y_temp\n","\n","\n","    for i in range(kwargs[\"Replicates\"]):\n","      # assign random integers in [1,K] to unassigned labels\n","      r = [i for i in range(k)]\n","\n","      ran_int = np.random.choice(r, size=(len(ind_unlabel),1))\n","\n","      y_temp[ind_unlabel] = ran_int\n","\n","      for j in range(kwargs[\"LearnerIter\"]):\n","        if j ==0:\n","          # first iteration need to split the y_temp for training etc.\n","          # use reset to add z_train, z_unlabel, y_temp_one_hot, to the dataset\n","          DataSets = DataSets.DataSets_reset(\"y_temp\")\n","          # Convert targets into one-hot encoded format\n","          y_temp_one_hot = to_categorical(y_temp)\n","          # initialize y_temp_one_hot in the first loop\n","          DataSets.y_temp_one_hot = y_temp_one_hot\n","        if j > 0:\n","          # update z_train, z_unlabel, and y_temp_train_one_hot to the dataset\n","          DataSets = DataSets.DataSets_reset(\"y_temp\")\n","        # all the gnn train on y_train_one_hot\n","        gnn = self.GNN_Direct(DataSets, DataSets.Y_train_one_hot)\n","        predict_probs = gnn.pred_probs\n","        pred_class = gnn.pred_class\n","        pred_class_prob = gnn.pred_class_prob\n","\n","        # z_unknown is initialized with none, so the pred_class may be none\n","        # This will not happen for the semi version,\n","        # since the unknown size should not be none for the semi version\n","        if type(pred_class) == np.ndarray:\n","          # if there are unkown labels and predicted labels are available\n","          # check if predicted_class are the same as the random integers\n","          # if so, stop the iteration in \"LearnerIter\" loop\n","          # shape (n,) is required for adjusted_rand_score()\n","          if adjusted_rand_score(ran_int.reshape((-1,)), pred_class) == 1:\n","            break\n","          # assign the probabilites for each class to the temp y_one_hot\n","          DataSets.y_temp_one_hot[ind_unlabel] = predict_probs\n","          # assgin the predicted classes to the temp Y unknown labels\n","          DataSets.y_temp[ind_unlabel, 0] = pred_class\n","          # # assign the highest possibility of the class to Y_temp\n","          # Y_temp[ind_unlabel, 0] = pred_class_prob\n","      minP = np.mean(pred_class_prob) - 3*np.std(pred_class_prob)\n","      if minP > meanSS:\n","        meanSS = minP\n","        Y = DataSets.y_temp\n","\n","      gnn.Y = Y\n","      gnn.meanSS = meanSS\n","      return gnn\n","\n","\n","  def GNN_complete(self):\n","    \"\"\"\n","      if LearnerIter set to False(<1):\n","        run GNN_Direct() with no iteration\n","      if LearnerIter set to True(>=1):\n","        run GNN_Iter(), which starts with radomly assigned k to unknown labels\n","\n","    \"\"\"\n","    kwargs = self.kwargs\n","\n","    DataSets = self.DataSets\n","    y_train = DataSets.Y_train\n","\n","\n","    if kwargs[\"LearnerIter\"] < 1:\n","      # Convert targets into one-hot encoded format\n","      y_train_one_hot = to_categorical(y_train)\n","      gnn = self.GNN_Direct(DataSets, y_train_one_hot)\n","    else:\n","      gnn = self.GNN_Iter(DataSets)\n","\n","    return gnn\n","############-----------------GNN_end-----------------------------###############\n","\n","############-----------------LDA_start---------------------------###############\n","class LDA:\n","  def __init__(self, DataSets, **kwargs):\n","    LDA.kwargs = self.kwargs_construct(**kwargs)\n","    LDA.DataSets = DataSets\n","    LDA.model = LinearDiscriminantAnalysis()  # asssume spseudolinear is its default setting\n","    LDA.meanSS = 0  # initialize the self-defined critirion meanSS\n","\n","  def kwargs_construct(self, **kwargs):\n","    defaultKwargs = {'Learner': 1,                         # LDA_Leaner\n","                     'LearnerIter': 0, \"Replicates\": 3     # LDA_Iter\n","                     }\n","    kwargs = { **defaultKwargs, **kwargs}  # update the args using input_args\n","    return kwargs\n","\n","  def LDA_Learner(self, DataSets):\n","    \"\"\"\n","      run this function when Learner set to 1.\n","      embedding via partial label, then learn unknown label via LDA.\n","    \"\"\"\n","    lda = copy.deepcopy(self)\n","    z_train = DataSets.z_train\n","    y_train = DataSets.Y_train\n","    ind_unlabel = DataSets.ind_unlabel\n","    z_unlabel = DataSets.z_unlabel\n","    Y = DataSets.Y\n","\n","    model = self.model\n","    model.fit(z_train,y_train)\n","    # train_acc = model.score(z_train,y_train)\n","\n","    # for semi-supervised learning\n","    if type(z_unlabel) == np.ndarray:\n","      # predict_probas include probabilities for all classes for each node\n","      pred_probs = model.predict_proba(z_unlabel)\n","      # assign the classes with the highest probability\n","      pred_class = model.predict(z_unlabel)\n","      # the corresponding probabilities of the predicted classes\n","      pred_class_prob = pred_probs[range(len(pred_probs)),pred_class]\n","      # assign the predicted class to Y\n","      Y[ind_unlabel, 0] = pred_class\n","      lda.Y = Y\n","      lda.pred_class = pred_class\n","      lda.pred_class_prob = pred_class_prob\n","\n","    lda.model = model\n","    return lda\n","\n","  def LDA_Iter(self):\n","    \"\"\"\n","      run this function when Learner set to 1, and LeanerIter is True(>=1)\n","      ramdonly assign labels to the unknownlabel.\n","      embedding via partial label, then learn unknown label via LDA.\n","    \"\"\"\n","\n","    kwargs = self.kwargs\n","    meanSS = self.meanSS\n","    DataSets = self.DataSets\n","\n","    k = DataSets.k\n","    Y = DataSets.Y\n","    ind_unlabel = DataSets.ind_unlabel\n","\n","    y_temp = np.copy(Y)\n","\n","    for i in range(kwargs[\"Replicates\"]):\n","      # assign random integers in [1,K] to unassigned labels\n","      r = [i for i in range(k)]\n","\n","      ran_int = np.random.choice(r, size=(len(ind_unlabel),1))\n","\n","      y_temp[ind_unlabel] = ran_int\n","\n","      DataSets.y_temp = y_temp\n","\n","      for j in range(kwargs[\"LearnerIter\"]):\n","        # use reset to add z_train, z_unlabel, to the dataset\n","        DataSets = DataSets.DataSets_reset(\"y_temp\")\n","        # all train on y_train\n","        lda = self.LDA_Learner(DataSets)\n","        pred_class = lda.pred_class\n","        pred_class_prob = lda.pred_class_prob\n","\n","        # z_unknown is initialized with none, so the pred_class may be none\n","        # This will not happen for the semi version,\n","        # since the unknown size should not be none for the semi version\n","        if type(pred_class) == np.ndarray:\n","          # if there are unkown labels and predicted labels are available\n","          # check if predicted_class are the same as the random integers\n","          # if so, stop the iteration in \"LearnerIter\" loop\n","          # shape (n,) is required for adjusted_rand_score()\n","          if adjusted_rand_score(ran_int.reshape((-1,)), pred_class) == 1:\n","            break\n","          # assgin the predicted classes to the temp Y unknown labels\n","          DataSets.y_temp[ind_unlabel, 0] = pred_class\n","          # # assign the highest possibility of the class to Y_temp\n","          # Y_temp[ind_unlabel, 0] = pred_class_prob\n","      minP = np.mean(pred_class_prob) - 3*np.std(pred_class_prob)\n","      if minP > meanSS:\n","        meanSS = minP\n","        Y = DataSets.y_temp\n","\n","      lda.Y = Y\n","      lda.meanSS = meanSS\n","      return lda\n","\n","\n","############-----------------LDA_end-----------------------------###############\n","\n","############------------Clustering_start-------------------------###############\n","class Clustering:\n","  \"\"\"\n","    The input DataSets.X is the s*3 edg list\n","    The innput DataSets.Y can be:\n","    1. A given cluster size, e.g. [3], meaning in total 3 clusters\n","    2. A range of cluster sizes. e.g. [3-5], meaning there are possibly 3 to 5 clusters\n","\n","  \"\"\"\n","  def __init__(self, DataSets, **kwargs):\n","    self.kwargs = self.kwargs_construct(**kwargs)\n","    self.DataSets = DataSets\n","    self.cluster_size_range = self.cluster_size_check()\n","    self.K = DataSets.Y[0]\n","\n","\n","  def kwargs_construct(self, **kwargs):\n","    defaultKwargs = {'Correlation': True,'MaxIter': 50, 'MaxIterK': 5,'Replicates': 3}\n","    kwargs = { **defaultKwargs, **kwargs}\n","    return kwargs\n","\n","  def cluster_size_check(self):\n","    DataSets = self.DataSets\n","    Y = DataSets.Y\n","\n","    cluster_size_range = None # in case that Y is an empty array.\n","\n","    if len(Y) == 1:\n","      cluster_size_range = False  # meaning the cluster size is known. e.g. [3]\n","    if len(Y) > 1:\n","      cluster_size_range = True   # meaning only know the range of cluster size. e.g. [2, 3, 4, 5]\n","\n","    return cluster_size_range\n","\n","  def graph_encoder_cluster(self, K):\n","    \"\"\"\n","      clustering function\n","    \"\"\"\n","    DataSets = self.DataSets\n","    X = DataSets.X\n","    n = DataSets.n\n","    kwargs = self.kwargs\n","    Encoder_kwargs = {k: kwargs[k] for k in ['Correlation']}\n","\n","\n","    minSS=-1\n","    Z = None\n","    W = None\n","\n","    for i in range(kwargs['Replicates']):\n","      Y_temp = np.random.randint(K,size=(n,1))\n","      for r in range(kwargs['MaxIter']):\n","        [Zt,Wt] = multi_graph_encoder_embed(DataSets, Y_temp, **Encoder_kwargs)\n","\n","        if DataSets.attributes:\n","          # add U to Z side by side\n","          Zt = np.concatenate((Zt, DataSets.U), axis=1)\n","        kmeans = KMeans(n_clusters=K, max_iter = kwargs['MaxIter']).fit(Zt)\n","        labels = kmeans.labels_ # shape(n,)\n","        # sum_in_cluster = kmeans.inertia_ # sum of distance within cluster (k,1)\n","        dis_to_centors = kmeans.transform(Zt)\n","        # adjusted_rand_score() needs the shape (n,)\n","        if adjusted_rand_score(Y_temp.reshape(-1,), labels) == 1:\n","          break\n","        else:\n","          # we need labels to be the same shape as for Y(n,1) when assign\n","          Y_temp = labels.reshape(-1,1)\n","\n","      # calculate score and compare with meanSS\n","      tmp = self.temp_score(dis_to_centors, K, labels, n)\n","      if (minSS == -1) or tmp < minSS:\n","        Z = Zt\n","        W = Wt\n","        minSS = tmp\n","        Y = labels\n","    return  Z, Y, W, minSS\n","\n","\n","  def temp_score(self, dis_to_centors, K, labels, n):\n","    \"\"\"\n","      calculate:\n","      1. sum_in_cluster(1*k): the sum of the distance from the nodes to the centroid\n","      of its belonged cluster\n","      2. sum_in_cluster_norm(1*k): normalize the sum_in_cluster by the\n","      corresponding label count (how many nodes in each cluster)\n","      3. sum_not_in_cluster(1*k): the sum of the distance of the cluster\n","      centroid to the nodes that do not belong to the cluster\n","      4. sum_not_in_cluster_norm(1*k): normalize the sum_other_centroids by the\n","      counts of the nodes that do not belong to the cluster.\n","      5. temp score(1*k):\n","      (normalized sum in cluster / normalized sum not in cluster ) *\n","      (label count in cluster / total node number)\n","      6. get mean + 2 standard deviation of temp score, then return\n","    \"\"\"\n","    label_count = np.bincount(labels)\n","    sum_in_cluster_squre = np.zeros((K,))\n","\n","    dis_to_centors_squre = dis_to_centors**2\n","\n","    for i in range(n):\n","      label = labels[i]\n","      sum_in_cluster_squre[label] += dis_to_centors_squre[i][label]\n","\n","    # how to find out if the distance is squared, the current method doesn't do square root.\n","    sum_not_in_cluster = (np.sum(dis_to_centors_squre, axis=0) - sum_in_cluster_squre)**0.5\n","\n","    sum_not_in_cluster_norm = sum_not_in_cluster/(n - label_count)\n","    sum_in_cluster_norm = sum_in_cluster_squre**0.5/label_count\n","\n","    tmp = sum_in_cluster_norm / sum_not_in_cluster_norm * label_count / n\n","    tmp = np.mean(tmp) + 2*np.std(tmp)\n","\n","    return tmp\n","\n","\n","  def cluster_main(self):\n","    K = self.K\n","    DataSets = self.DataSets\n","    X = DataSets.X\n","    n = DataSets.n\n","\n","    kmax = np.amax(K)\n","    if n/kmax < 30:\n","      print('Too many clusters at maximum. Result may bias towards large K. Please make sure n/Kmax >30.')\n","    # when the cluster size is specified\n","    if not self.cluster_size_range:\n","      [Z,Y,W,meanSS]= self.graph_encoder_cluster(K[0])\n","    # when the range of cluster size is provided\n","    # columns are less than n/2 and kmax is less than max(n/2, 10)\n","    if self.cluster_size_range:\n","      k_range = len(K)\n","      if k_range < n/2 and kmax < max(n/2, 10):\n","          minSS = -1\n","          Z = 0\n","          W = 0\n","          meanSS = np.zeros((k_range,1))\n","          for i in range(k_range):\n","            [Zt,Yt,Wt,tmp]= self.graph_encoder_cluster(K[i])\n","            meanSS[i,0] = i\n","            if (minSS == -1) or tmp < minSS:\n","              minSS = tmp\n","              Y = Yt\n","              Z = Zt\n","              W = Wt\n","    return Z, Y, W, meanSS\n","\n","############------------Clustering_end---------------------------###############\n","############------------Evaluation_start---------------------------#############\n","class Evaluation:\n","  def GNN_supervise_test(self, gnn, z_test, y_test):\n","    \"\"\"\n","      test the accuracy for GNN_direct\n","    \"\"\"\n","    y_test_one_hot = to_categorical(y_test)\n","    # set verbose to 0 to silent the output\n","    test_loss, test_acc = gnn.model.evaluate(z_test,  y_test_one_hot, verbose=0)\n","\n","    return test_acc\n","\n","  def LDA_supervise_test(self, lda, z_test, y_test):\n","    \"\"\"\n","      test the accuracy for LDA_learner\n","    \"\"\"\n","    test_acc = lda.model.score(z_test, y_test)\n","\n","    return test_acc\n","\n","  def GNN_semi_supervised_learn_test(self,Y_result, Y_original):\n","    \"\"\"\n","      test accuracy for semi-supervised learning\n","    \"\"\"\n","    test_acc = metrics.accuracy_score(Y_result, Y_original)\n","\n","    return test_acc\n","\n","  def GNN_semi_supervised_not_learn_test(self, gnn, Dataset, case):\n","    \"\"\"\n","      test accuracy for semi-supervised learning\n","    \"\"\"\n","\n","    ind_unlabel = Dataset.ind_unlabel\n","    z_unlabel =  Dataset.z_unlabel\n","    y_unlabel_ori = case.Y_ori[ind_unlabel, 0]\n","    y_unlabel_ori_one_hot = to_categorical(y_unlabel_ori)\n","    test_loss, test_acc = gnn.model.evaluate(z_unlabel, y_unlabel_ori_one_hot, verbose=0)\n","\n","    return test_acc\n","\n","\n","  def clustering_test(self, Y_result, Y_original):\n","    \"\"\"\n","      test accuracy for semi-supervised learning\n","    \"\"\"\n","    ari = adjusted_rand_score(Y_result, Y_original.reshape(-1,))\n","\n","    return ari\n","\n","############-----------------Matrix_conversion-------------------###############\n","def To_multi_sparse_matrix(M_list,option):\n","  M_list_new = []\n","  for M in M_list:\n","    M_new = To_single_sparse_matrix(M,option)\n","    M_list_new.append(M_new)\n","\n","  return M_list_new\n","\n","def To_single_sparse_matrix(M,option):\n","  \"\"\"\n","    coo_matrix is efficient and fast to construct,\n","      However, arithmetic operations are not efficient on this matrix.\n","      One can easily convert coo_matrix to csc_matrix/csr_matrix\n","    csc_matrix/csr_matrix are efficient in column_slicing/row_slicing,\n","      One can have efficient multiplication or inversion.\n","  \"\"\"\n","  if option == \"coo\":\n","    M = sparse.coo_matrix(M)\n","  if option == \"csr\":\n","    M = sparse.csr_matrix(M)\n","  if option == \"csc\":\n","    M = sparse.csc_matrix(M)\n","\n","  return M\n","\n"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["wVQ_Rwe9PClj"],"authorship_tag":"ABX9TyO+ln+wLTrZvX/AQYOPXjpR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}