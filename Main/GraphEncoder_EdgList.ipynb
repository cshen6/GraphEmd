{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdFl3x2SBBFf"
      },
      "source": [
        "#Package Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1feGDRiBv-2-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import copy\n",
        "from numpy import linalg as LA\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn import metrics\n",
        "import time\n",
        "# for sparse matrix\n",
        "from scipy import sparse\n",
        "#early stop\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classes and functions"
      ],
      "metadata": {
        "id": "rdtkqKg_nhmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# invalide devide resutls will be nan\n",
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "############------------graph_encoder_embed_start----------------###############\n",
        "class GraphEncoderEmbed:\n",
        "  def run(self, X, Y, n, **kwargs):\n",
        "    defaultKwargs = {'DiagA': True, 'Laplacian': False, 'Correlation': True}\n",
        "    kwargs = { **defaultKwargs, **kwargs}\n",
        "    \n",
        "    X = self.to_s3_list(X)\n",
        "    \n",
        "    emb_strat = time.time()\n",
        "\n",
        "    if kwargs['DiagA']:\n",
        "      X = self.Diagonal(X, n)\n",
        "\n",
        "    if kwargs['Laplacian']:\n",
        "      X = self.Laplacian(X, n)\n",
        "    \n",
        "    Z, W = self.Basic(X, Y, n)\n",
        "\n",
        "    if kwargs['Correlation']:\n",
        "      Z = self.Correlation(Z)\n",
        "    \n",
        "    emb_end = time.time()\n",
        "    emb_time = emb_end - emb_strat\n",
        "    \n",
        "    return Z, W, emb_time\n",
        "\n",
        "  def Basic(self, X, Y, n):\n",
        "    \"\"\"\n",
        "      graph embedding basic function\n",
        "      input X is S3 edge list\n",
        "      input Y is numpy array with size (n,1):\n",
        "      -- value -1 indicate no lable\n",
        "      -- value >=0 indicate real label\n",
        "      input n: number of vertices\n",
        "    \"\"\"\n",
        "    k = Y[:,0].max() + 1\n",
        "\n",
        "    Z = np.zeros((n,k))   \n",
        "    #nk: 1*n array, contains the number of observations in each class\n",
        "    #W: encoder marix. W[i,k] = {1/nk if Yi==k, otherwise 0}\n",
        "    nk = np.zeros((1,k))\n",
        "    W = np.zeros((n,k))\n",
        "\n",
        "    for i in range(k):\n",
        "      nk[0,i] = np.count_nonzero(Y[:,0]==i)\n",
        "\n",
        "    for i in range(Y.shape[0]):\n",
        "      k_i = Y[i,0]\n",
        "      if k_i >=0:\n",
        "        W[i,k_i] = 1/nk[0,k_i] ## GEE paper\n",
        "        # W[i,k_i] = nk[0,k_i]/n ## use as an example to show if people want to use nk/n instead of 1/nk\n",
        " \n",
        "    for row in X:\n",
        "      [v_i, v_j, edg_i_j] = row\n",
        "      v_i = int(v_i)\n",
        "      v_j = int(v_j)\n",
        "\n",
        "      label_i = Y[v_i][0] \n",
        "      label_j = Y[v_j][0]\n",
        "\n",
        "      if label_j >= 0:\n",
        "        Z[v_i, label_j] = Z[v_i, label_j] + W[v_j, label_j]*edg_i_j\n",
        "      if (label_i >= 0) and (v_i != v_j):\n",
        "        Z[v_j, label_i] = Z[v_j, label_i] + W[v_i, label_i]*edg_i_j\n",
        "\n",
        "    return Z, W\n",
        "\n",
        "  def Diagonal(self, X, n):\n",
        "    # add self-loop to edg list -- add 1 connection for each (i,i)\n",
        "    self_loops = np.column_stack((np.arange(n), np.arange(n), np.ones(n)))\n",
        "    # faster than vstack --  adding the second to the bottom\n",
        "    X = np.concatenate((X,self_loops), axis = 0)\n",
        "    return X\n",
        "\n",
        "  def Laplacian(self, X, n):\n",
        "    s = X.shape[0] # get the row number of the edg list\n",
        "\n",
        "    D = np.zeros((n,1))\n",
        "    for row in X:\n",
        "      [v_i, v_j, edg_i_j] = row\n",
        "      v_i = int(v_i)\n",
        "      v_j = int(v_j)\n",
        "      D[v_i] = D[v_i] + edg_i_j\n",
        "      if v_i != v_j:\n",
        "        D[v_j] = D[v_j] + edg_i_j\n",
        "\n",
        "    D = np.power(D, -0.5)\n",
        "    \n",
        "    for i in range(s):\n",
        "      X[i,2] = X[i,2] * D[int(X[i,0])] * D[int(X[i,1])]\n",
        "\n",
        "    return X\n",
        "  \n",
        "  def Correlation(self, Z):\n",
        "    \"\"\"\n",
        "      Calculate each row's 2-norm (Euclidean distance). \n",
        "      e.g.row_x: [ele_i,ele_j,ele_k]. norm2 = sqr(sum(ele_i^2+ele_i^2+ele_i^2))\n",
        "      then divide each element by their row norm\n",
        "      e.g. [ele_i/norm2,ele_j/norm2,ele_k/norm2]\n",
        "    \"\"\"\n",
        "    row_norm = LA.norm(Z, axis = 1)\n",
        "    reshape_row_norm = np.reshape(row_norm, (n,1))\n",
        "    Z = np.nan_to_num(Z/reshape_row_norm)\n",
        "\n",
        "    return Z\n",
        "    \n",
        "  def to_s3_list(self,X):\n",
        "    \"\"\"\n",
        "      if input X only has 2 columns, make it into s3 edge list.\n",
        "      this function will return a s3 edge list\n",
        "      [node_i, node_j, weight]...\n",
        "    \"\"\"\n",
        "    s = X.shape[0] # get the row number of the edg list\n",
        "    if X.shape[1] == 2:\n",
        "      # enlarge the edgelist to s*3 by adding 1 to the thrid position as adj(i,j)\n",
        "      X = np.insert(X, 1, np.ones(s,1))\n",
        "\n",
        "    return X\n",
        "\n",
        "############------------graph_encoder_embed_end------------------###############\n",
        "############------------Sparse_supervised_learning_start---------###############\n",
        "\n",
        "# https://www.kaggle.com/c/talkingdata-mobile-user-demographics/discussion/22567\n",
        "# https://github.com/tkipf/pygcn/blob/1600b5b748b3976413d1e307540ccc62605b4d6d/pygcn/utils.py#L73\n",
        "\n",
        "def batch_generator(X, y, k, batch_size, shuffle):\n",
        "    number_of_batches = int(X.shape[0]/batch_size)\n",
        "    counter = 0\n",
        "    sample_index = np.arange(X.shape[0])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "    while True:\n",
        "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
        "        X_batch = X[batch_index,:]\n",
        "        y_batch = to_categorical(y[batch_index], num_classes=k)\n",
        "        counter += 1\n",
        "        yield X_batch, y_batch\n",
        "        if (counter == number_of_batches):\n",
        "            if shuffle:\n",
        "                np.random.shuffle(sample_index)\n",
        "            counter = 0\n",
        "\n",
        "class Hyperperameters:\n",
        "  \"\"\"\n",
        "    define perameters for GNN.\n",
        "    default values are for GNN learning -- \"Leaner\" ==2:\n",
        "      embedding via partial label, then learn unknown label via two-layer NN\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # there is no scaled conjugate gradiant in keras optimiser, use defualt instead\n",
        "    # use whatever default\n",
        "    self.learning_rate = 0.01  # Initial learning rate.\n",
        "    self.epochs = 100 #Number of epochs to train.\n",
        "    self.hidden = 20 #Number of units in hidden layer \n",
        "    self.val_split = 0.1 #Split 10% of training data for validation\n",
        "    self.loss = 'categorical_crossentropy' # loss function\n",
        "\n",
        "class GNN:\n",
        "  def __init__(self, DataSets):\n",
        "    GNN.DataSets = DataSets\n",
        "    GNN.hyperM = Hyperperameters()\n",
        "    GNN.model = self.GNN_model()  #model summary: GNN.model.summary()\n",
        "      \n",
        " \n",
        "  def GNN_model(self):\n",
        "    \"\"\"\n",
        "      build GNN model\n",
        "    \"\"\"\n",
        "    hyperM = self.hyperM\n",
        "    DataSets = self.DataSets\n",
        "\n",
        "    z_train = DataSets.z_train\n",
        "    k = DataSets.d\n",
        "\n",
        "    feature_num = z_train.shape[1]\n",
        "    \n",
        "    model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape = (feature_num,)),  # input layer \n",
        "    keras.layers.Dense(hyperM.hidden, activation='relu'),  # hidden layer -- no tansig activation function in Keras, use relu instead\n",
        "    keras.layers.Dense(k, activation='softmax') # output layer, matlab used softmax for patternnet default ??? max(opts.neuron,K)? opts \n",
        "    ])\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = hyperM.learning_rate)\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=hyperM.loss,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "    \n",
        "  def GNN_run(self, flag):\n",
        "    \"\"\"\n",
        "      Train and test directly.\n",
        "      Do not learn from the unknown labels.\n",
        "    \"\"\"\n",
        "    gnn = copy.deepcopy(self)\n",
        "    hyperM = gnn.hyperM\n",
        "    DataSets = self.DataSets\n",
        "    k = DataSets.d\n",
        "    z_train = DataSets.z_train\n",
        "    y_train = DataSets.y_train\n",
        "    y_test = DataSets.y_test\n",
        "    z_test = DataSets.z_test\n",
        "    model = gnn.model    \n",
        "\n",
        "\n",
        "    if flag == \"direct\":\n",
        "      y_train_one_hot = to_categorical(y_train, num_classes=k)\n",
        "      train_strat = time.time() \n",
        "      history = model.fit(z_train, y_train_one_hot, \n",
        "        validation_split=hyperM.val_split,\n",
        "        epochs=hyperM.epochs, \n",
        "        shuffle=True,\n",
        "        verbose=0)\n",
        "    else:\n",
        "      early_stopping_callback = EarlyStopping(monitor='loss', patience=5, verbose=0)\n",
        "      checkpoint_callback = ModelCheckpoint('GNN.h5', monitor='loss', save_best_only=True, mode='min', verbose=0)\n",
        "      \n",
        "      train_strat = time.time()\n",
        "      history = model.fit(batch_generator(z_train, y_train, k, 32, True),\n",
        "                      epochs=hyperM.epochs,\n",
        "                      steps_per_epoch=z_train.shape[0],\n",
        "                      callbacks=[early_stopping_callback, checkpoint_callback],\n",
        "                      verbose=0)\n",
        "    train_end = time.time()\n",
        "    train_time = train_end - train_strat \n",
        "\n",
        "    y_test_one_hot = to_categorical(y_test, num_classes=k) \n",
        "    # set verbose to 0 to silent the output\n",
        "    test_loss, test_acc = gnn.model.evaluate(z_test,  y_test_one_hot, verbose=0) \n",
        "    return test_acc, train_time\n",
        "############------------Sparse_supervised_learning_end---------###############\n"
      ],
      "metadata": {
        "id": "d60qfGLDnY2q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}